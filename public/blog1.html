<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Is AI Eating Its Own Tail? Embracing 'AI Veganism' to Keep AI Healthy</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        .container {
            max-width: 800px;
            margin: 20px auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1 {
            font-size: 2em;
            margin-bottom: 20px;
            color: #007acc;
        }
        h2 {
            font-size: 1.5em;
            margin-bottom: 15px;
            color: #005a9e;
        }
        p {
            margin-bottom: 20px;
        }
        code {
            background-color: #eee;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: "Courier New", Courier, monospace;
        }
        pre {
            background-color: #eee;
            padding: 10px;
            border-radius: 3px;
            overflow-x: auto;
            margin-bottom: 20px;
        }
        .collapsible {
            background-color: #007acc;
            color: white;
            cursor: pointer;
            padding: 10px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 15px;
            border-radius: 4px;
            margin-top: 20px;
        }
        .collapsible:after {
            content: '\002B'; /* Plus sign */
            font-weight: bold;
            float: right;
            margin-left: 5px;
        }
        .active:after {
            content: "\2212"; /* Minus sign */
        }
        .content {
            padding: 0 18px;
            display: none;
            overflow: hidden;
            background-color: #f9f9f9;
            border-radius: 4px;
            margin-top: 10px;
        }
        .figure {
            text-align: center;
            margin-bottom: 20px;
        }
        .figure img {
            border: 1px solid #ccc;
            border-radius: 4px;
        }
        .figure-caption {
            font-size: 0.9em;
            color: #555;
        }
        .back-button {
            display: inline-block;
            padding: 10px 20px;
            background-color: #007acc;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            margin-bottom: 20px;
        }
    </style>
    <!-- Include MathJax for rendering math -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-button">← Back to Main Page</a>
        <h1>Is AI Eating Its Own Tail? On 'AI Veganism' and how it could save our models and ourselves</h1>
        
        <p>In 2024, social media has changed in not so great ways. For me, it's been a wave of AI-generated stuff, people talking on AI, and, well, a whole lot more of AI stuff. Combine that with the ongoing political drama and a noticeable surge in right-wing commentary, often delivered through memes (<a href="https://www.reuters.com/world/us/despair-makes-young-us-men-more-conservative-ahead-us-election-poll-shows-2024-04-12/">1</a>), and it's clear something has shifted. I'm not sure exacly how, but I think AI is influencing this. A week ago, I came across a photo of Ouroboros — a depiction of a snake eating its own tail seen in ancient texts — and it got me thinking: Could AI end up hurting us in the long term by learning from content created by itself and perpetuating the cycle?</p>
        
        <h2>The Ouroboros Effect: When AI Starts Consuming Itself</h2>
        <div style="text-align: center;">
            <img src="440px-Serpiente_alquimica.jpg" alt="Ouroboros" style="max-width: 100%; height: auto;">
            <p><small>Image source: <a href="https://en.wikipedia.org/wiki/Ouroboros">Wikipedia</a></small></p>
        </div>
        <p>This ancient symbol is a fitting metaphor for what might happen if AI starts learning from AI-generated content. The risk is a feedback loop where mistakes and biases get amplified, causing the quality of AI outputs to decline over time. Eventually, the clarity fades.</p>

        <button type="button" class="collapsible">The Mathematical Importance of Diversity and Entropy in Training Data</button>
        <div class="content">
            <p>Entropy, in simple terms, is a way to measure diversity or unpredictability. High entropy means lots of variety; low entropy means things are more predictable. When AI starts learning from other AI-generated content, the overall entropy of the training data can decrease, leading to less creative and more repetitive outputs. <br> 
            Diversity in training data is crucial for AI models to generalize well and produce innovative outputs. But, though entropy measures the unpredictability of a dataset, it doesn't fully capture the interactions between human-generated and AI-generated content. The entropy of a mixture of two sources isn't simply the weighted sum of their individual entropies.</p>

            <p> A better way to get at total entropy would be:</p>

            <pre>H<sub>total</sub> = H((1 - γ(t)) P + γ(t) Q)</pre>

            <p>Where P and Q are the probability distributions of human-generated and AI-generated content. This requires understanding how the distributions overlap, which may be challenging and I'll get to that later</p>

            <p>The assumption that the redundancy term D ≈ 0 should also be justified, as AI content may mimic human patterns and affect entropy in complex ways. Using empirical data to estimate the actual entropy of combined datasets would strengthen the argument.</p>
        </div>

        <h2>Modeling the Growth of AI-Generated Content: A Logistic Approach</h2>
        <p>To assess the risk, we need to model the projected growth of AI-generated content online. Let's consider some data points from <a href="https://originality.ai/ai-content-in-google-search-results">Originality.ai</a>:</p>
        <ul>
            <li><strong>2019 (t = 0)</strong>: \(\gamma(0) = 0.023\)</li>
            <li><strong>March 2024 (t = 5.25)</strong>: \(\gamma(5.25) = 0.102\)</li>
            <li><strong>June 2024 (t = 5.5)</strong>: \(\gamma(5.5) = 0.1395\)</li>
            <li><strong>August 2024 (t = 5.67)</strong>: \(\gamma(5.67) = 0.1308\)</li>
        </ul>
        <p>Using this data, we can estimate the parameters of a logistic growth model:</p>

        <blockquote>
            The logistic growth function is:
            \[
            \gamma(t) = \frac{K}{1 + e^{-r(t - t_0)}}
            \]
        </blockquote>

        <p>Where:</p>
        <ul>
            <li>\(\gamma(t)\) is the proportion of AI-generated content at time \(t\).</li>
            <li>\(K\) is the carrying capacity (maximum limit, set to 1 for 100%).</li>
            <li>\(r\) is the growth rate.</li>
            <li>\(t_0\) is the inflection point (the time when growth is fastest).</li>
        </ul>
        
        <h2>Fitting the Logistic Model</h2>
        <p>Using nonlinear regression and tools like Python's SciPy library, we estimate:</p>
        <ul>
            <li><strong>r ≈ 0.8</strong> per year</li>
            <li><strong>t<sub>0</sub> ≈ 6</strong> years (mid-2025)</li>
        </ul>


        <h2>Google Search Results: a prediction</h2>
        <p>To find when AI-generated content will make up 50% of the internet, we set \(\gamma(t) = 0.5\) and solve:</p>

        <blockquote>
            \[
            t = 6 \text{ years}
            \]
        </blockquote>

        <p><strong>Conclusion:</strong> The model predicts that AI-generated content will reach 50% around mid-2025.</p>

        <h2>What about 90%?</h2>
        <p>For 90%, we solve for \(\gamma(t) = 0.9\):</p>

        <blockquote>
            \[
            t ≈ 8.746 \text{ years}
            \]
        </blockquote>

        <p><strong>Conclusion:</strong> AI-generated content could reach 90% around late 2027.</p>

        <h2>The Mathematical Impact on AI Models: Entropy Reduction</h2>
        <p>To quantify the impact on entropy, we consider the combined entropy of human and AI-generated content:</p>

        <blockquote>
            \[
            H_{\text{total}} = (1 - \gamma(t)) H_{\text{human}} + \gamma(t) H_{\text{AI}}
            \]
        </blockquote>

        <p>For simplicity, let's assume:</p>
        <ul>
            <li>H<sub>human</sub> = 10 bits (high diversity)</li>
            <li>H<sub>AI</sub> = 8 bits (lower diversity)</li>
        </ul>

        <p>At t = 6 years (50% AI content):</p>

        <blockquote>
            \[
            H_{\text{total}} = 0.5 \times 10 + 0.5 \times 8 = 9 \text{ bits}
            \]
        </blockquote>

        <p>At t = 8.746 years (90% AI content):</p>

        <blockquote>
            \[
            H_{\text{total}} = 0.1 \times 10 + 0.9 \times 8 = 8.2 \text{ bits}
            \]
        </blockquote>

        <p>The entropy decreases from 10 bits to 8.2 bits as the proportion of AI-generated content increases from 0% to 90%, indicating a reduction in diversity.</p>

        <h2>What if... we made a 'Human-Only' browser</h2>
        <p>I've been thinking a lot about this -- imagine browsing the web with the assurance that every article, every blog post, every snippet of text was crafted by a human hand. It would serve AI well, for models like o1 are based on pre-2021 data. But, if the sanity of the data from now till 2028 is, well, garbage, AI could be in trouble. <br> On a more personal and <i>human</i> note, I think it would help creatives as well (artists, writers, etc). AI also has biases, as we know. Letting it train itself on AI-generated content could be harmful. <br> This is why I think a 'Human-Only' browser is a good solution. It would help AI stay healthy, and it would help creators stay healthy. Everyone wins. </p>
        
        <p>Without a sanity check -- without a way to ensure that our future AI systems are grounded in authentic human insight -- we risk drifting into a future where AI learning as well as regular learning is compromised. </p>
            <p>We can use the <strong>Kullback-Leibler (KL) divergence</strong> to measure the difference between the human-generated content distribution \(P\) and the AI-generated content distribution \(Q\):</p>
            
            <blockquote>
                \[
                D_{\text{KL}}(P \parallel Q) = \sum_{x} P(x) \log_2 \left(\frac{P(x)}{Q(x)}\right)
                \]
            </blockquote>
            
            <p>As \(\gamma(t)\) increases, the training data shifts from \(P\) towards \(Q\), increasing the divergence and potentially degrading model performance.</p>


        <h2>Sanity Checks for the Internet</h2>
        <h3>1. Advanced Filtering Algorithms</h3>
        <p>Develop machine learning models to detect AI-generated content by analyzing unique writing styles and identifying statistical anomalies.</p>
        <h3>2. Dynamic Entropy Monitoring</h3>
        <p>Continuously monitor entropy levels in training data and set thresholds to maintain data diversity.</p>


        <div class="references">
            <h2>References</h2>
            <ol>
                <li>Shannon, C. E. (1948). "A Mathematical Theory of Communication." Introduces the foundational concepts of information theory and entropy.</li>
                <li>Anonymous (2023). "The Curse of Recursion: Training on Generated Data Makes Models Forget." Discusses the phenomenon of model collapse due to training on AI-generated data.</li>
                <li>Kullback, S., & Leibler, R. A. (1951). "On Information and Sufficiency." Presents the KL divergence measure for statistical distributions.</li>
            </ol>
        </div>
        
        <div class="further-reading">
            <h2>Further Reading</h2>
            <ul>
                <li>Goodfellow, I., et al. (2014). "Generative Adversarial Networks." Explores GANs, which can be used for generating and detecting AI-generated content.</li>
                <li>Radford, A., et al. (2019). "Language Models are Unsupervised Multitask Learners." Discusses the capabilities and limitations of large-scale language models.</li>
            </ul>
        </div>
        
        <footer>
            <p>By embracing a mathematically informed approach to AI development, we can safeguard against the risks of self-referential training and ensure that AI continues to be a tool that enhances human creativity and innovation.</p>
        </footer>
    </div>

    <script>
        const collapsibles = document.querySelectorAll(".collapsible");
        collapsibles.forEach((collapsible) => {
            collapsible.addEventListener("click", function() {
                this.classList.toggle("active");
                const content = this.nextElementSibling;
                if (content.style.display === "block") {
                    content.style.display = "none";
                } else {
                    content.style.display = "block";
                }
            });
        });
    </script>
</body>
</html>
